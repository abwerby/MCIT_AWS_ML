{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Term Frequency \n",
    "\n",
    "* We will use Sklearn's __CountVectorizer()__ function with __binary=True__ parameter. \n",
    "* Let's talk about __fit()__ and __transform()__ functions. \n",
    "    * __fit()__ function \"fits\" the text data and creates the vocabulary on the text. \n",
    "    * __transform()__ function calculates the needed vector numbers. They are, in this case, binary values 0s or 1s. \n",
    "* We can get the feature vectors with this: __features.toarray()__ This gives you a multi-dim numpy array.\n",
    "* Then, we will put the result into a data frame for nice result visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "texts = [\"good movie\", \"bad acting\", \"it was boring movie\"]\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectorizer.fit(texts)\n",
    "features = vectorizer.transform(texts)\n",
    "\n",
    "df = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counts\n",
    "\n",
    "* We will use Sklearn's __CountVectorizer()__ function. \n",
    "* __fit()__ function \"fits\" the text data and creates the vocabulary on the text.\n",
    "* __transform()__ function calculates the needed vector numbers. They are raw token/word counts for this example.\n",
    "* We can get the feature vectors with this: __features.toarray()__ This gives you a multi-dim numpy array.\n",
    "* Then, we will put the result into a data frame for nice result visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "texts = [\"good movie\", \"bad acting\", \"it was boring movie\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(texts)\n",
    "features = vectorizer.transform(texts)\n",
    "\n",
    "df = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams \n",
    "We can use N-grams in our feature vectors. This will help us getting token sequences. The example below uses 1-grams (regular tokens) and 2-grams (2 consecutive tokens). Pay attention to our new vocabulary, it gest __BIGGER__ because of additional 2-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "texts = [\"good movie\", \"bad acting\", \"it was boring movie\"]\n",
    "\n",
    "# CountVectorizer(ngram_range = (ngram_low_limit, ngram_up_limit)) \n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "vectorizer.fit(texts)\n",
    "features = vectorizer.transform(texts)\n",
    "\n",
    "df = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequencies\n",
    "\n",
    "* We will use Sklearn's __TfidfVectorizer()__ function with __use_idf=False__ parameter.\n",
    "* fit() function \"fits\" the text data and creates the vocabulary on the text.\n",
    "* transform() function calculates term frequencies. SKlearn automatically applies __L2 normalization__ to each vector.\n",
    "* We can get the feature vectors with this: features.toarray() This gives you a multi-dim numpy array.\n",
    "* Then, we will put the result into a data frame for nice result visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "texts = [\"good movie\", \"bad acting\", \"it was boring movie\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=False)\n",
    "vectorizer.fit(texts)\n",
    "features = vectorizer.transform(texts)\n",
    "\n",
    "df = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF\n",
    "\n",
    "* We will use Sklearn's __TfidfVectorizer()__ function.\n",
    "* fit() function \"fits\" the text data and creates the vocabulary on the text.\n",
    "* transform() function calculates term freq. - inverse document frequencies (TF-IDF). SKlearn automatically applies __L2 normalization__ to each vector.\n",
    "* We can get the feature vectors with this: features.toarray() This gives you a multi-dim numpy array.\n",
    "* Then, we will put the result into a data frame for nice result visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "texts = [\"good movie\", \"bad acting\", \"it was boring movie\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(texts)\n",
    "features = vectorizer.transform(texts)\n",
    "\n",
    "df = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing with limited feature size (smaller vocabulary)\n",
    "\n",
    "Sometimes we may need to reduce the size of our feature array for faster training and better generalization. In this case, we can use the __max_features__ parameter. This will keep the most important features by the given feature size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "texts = [\"good movie\", \"bad acting\", \"it was boring movie\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=3)    # TF-IDF \n",
    "#vectorizer = CountVectorizer(max_features=3)  # Word counts\n",
    "\n",
    "vectorizer.fit(texts)\n",
    "features = vectorizer.transform(texts)\n",
    "\n",
    "df = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
